{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lavis.models.blip2_models.modeling_llama'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m LlamaTokenizer\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlavis\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mblip2_models\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_llama\u001b[39;00m \u001b[39mimport\u001b[39;00m LlamaForCausalLM\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lavis.models.blip2_models.modeling_llama'"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "from lavis.models.blip2_models.modeling_llama import LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo chmod -R u+rwx /home/ubuntu/.cache/huggingface/hub/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-03 12:54:04 llm_engine.py:70] Initializing an LLM engine with config: model='meta-llama/Llama-2-7b-hf', tokenizer='meta-llama/Llama-2-7b-hf', tokenizer_mode=auto, trust_remote_code=False, dtype=torch.float16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)\n",
      "INFO 08-03 12:54:04 tokenizer.py:29] For some LLaMA-based models, initializing the fast tokenizer may take a long time. To eliminate the initialization time, consider using 'hf-internal-testing/llama-tokenizer' instead of the original tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/vllm/env/lib/python3.8/site-packages/torch/cuda/__init__.py:173: UserWarning: \n",
      "NVIDIA H100 PCIe with CUDA capability sm_90 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70 sm_75 sm_80 sm_86.\n",
      "If you want to use the NVIDIA H100 PCIe GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[39m=\u001b[39m LLM(model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmeta-llama/Llama-2-7b-hf\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/vllm/vllm/entrypoints/llm.py:66\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, trust_remote_code, tensor_parallel_size, dtype, seed, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdisable_log_stats\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     56\u001b[0m engine_args \u001b[39m=\u001b[39m EngineArgs(\n\u001b[1;32m     57\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     58\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     65\u001b[0m )\n\u001b[0;32m---> 66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_engine \u001b[39m=\u001b[39m LLMEngine\u001b[39m.\u001b[39;49mfrom_engine_args(engine_args)\n\u001b[1;32m     67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_counter \u001b[39m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/vllm/vllm/engine/llm_engine.py:220\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args)\u001b[0m\n\u001b[1;32m    217\u001b[0m distributed_init_method, placement_group \u001b[39m=\u001b[39m initialize_cluster(\n\u001b[1;32m    218\u001b[0m     parallel_config)\n\u001b[1;32m    219\u001b[0m \u001b[39m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m engine \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49mengine_configs,\n\u001b[1;32m    221\u001b[0m              distributed_init_method,\n\u001b[1;32m    222\u001b[0m              placement_group,\n\u001b[1;32m    223\u001b[0m              log_stats\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m engine_args\u001b[39m.\u001b[39;49mdisable_log_stats)\n\u001b[1;32m    224\u001b[0m \u001b[39mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/vllm/vllm/engine/llm_engine.py:101\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, distributed_init_method, placement_group, log_stats)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_workers_ray(placement_group)\n\u001b[1;32m    100\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_workers(distributed_init_method)\n\u001b[1;32m    103\u001b[0m \u001b[39m# Profile the memory usage and initialize the cache.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_cache()\n",
      "File \u001b[0;32m~/vllm/vllm/engine/llm_engine.py:133\u001b[0m, in \u001b[0;36mLLMEngine._init_workers\u001b[0;34m(self, distributed_init_method)\u001b[0m\n\u001b[1;32m    125\u001b[0m worker \u001b[39m=\u001b[39m Worker(\n\u001b[1;32m    126\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_config,\n\u001b[1;32m    127\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m     distributed_init_method,\n\u001b[1;32m    131\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers\u001b[39m.\u001b[39mappend(worker)\n\u001b[0;32m--> 133\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_workers(\n\u001b[1;32m    134\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39minit_model\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    135\u001b[0m     get_all_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    136\u001b[0m )\n",
      "File \u001b[0;32m~/vllm/vllm/engine/llm_engine.py:470\u001b[0m, in \u001b[0;36mLLMEngine._run_workers\u001b[0;34m(self, method, get_all_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    468\u001b[0m         executor \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(worker, method)\n\u001b[0;32m--> 470\u001b[0m     output \u001b[39m=\u001b[39m executor(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    471\u001b[0m     all_outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m    473\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_config\u001b[39m.\u001b[39mworker_use_ray:\n",
      "File \u001b[0;32m~/vllm/vllm/worker/worker.py:67\u001b[0m, in \u001b[0;36mWorker.init_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m# Initialize the model.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m set_random_seed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_config\u001b[39m.\u001b[39mseed)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m get_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_config)\n",
      "File \u001b[0;32m~/vllm/vllm/model_executor/model_loader.py:46\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_config)\u001b[0m\n\u001b[1;32m     42\u001b[0m torch\u001b[39m.\u001b[39mset_default_dtype(model_config\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m     44\u001b[0m \u001b[39m# Create a model instance.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m# The weights will be initialized as empty tensors.\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m model \u001b[39m=\u001b[39m model_class(model_config\u001b[39m.\u001b[39;49mhf_config)\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m model_config\u001b[39m.\u001b[39muse_dummy_weights:\n\u001b[1;32m     48\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/vllm/vllm/model_executor/models/llama.py:236\u001b[0m, in \u001b[0;36mLlamaForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[0;32m--> 236\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m LlamaModel(config)\n\u001b[1;32m    237\u001b[0m vocab_size \u001b[39m=\u001b[39m ((config\u001b[39m.\u001b[39mvocab_size \u001b[39m+\u001b[39m \u001b[39m63\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m64\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m    238\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m ColumnParallelLinear(config\u001b[39m.\u001b[39mhidden_size,\n\u001b[1;32m    239\u001b[0m                                     vocab_size,\n\u001b[1;32m    240\u001b[0m                                     bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    241\u001b[0m                                     gather_output\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    242\u001b[0m                                     perform_initialization\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/vllm/vllm/model_executor/models/llama.py:200\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    197\u001b[0m vocab_size \u001b[39m=\u001b[39m ((config\u001b[39m.\u001b[39mvocab_size \u001b[39m+\u001b[39m \u001b[39m63\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m64\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m    198\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m VocabParallelEmbedding(\n\u001b[1;32m    199\u001b[0m     vocab_size, config\u001b[39m.\u001b[39mhidden_size, perform_initialization\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 200\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([\n\u001b[1;32m    201\u001b[0m     LlamaDecoderLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    202\u001b[0m ])\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39m=\u001b[39m RMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/vllm/vllm/model_executor/models/llama.py:201\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    197\u001b[0m vocab_size \u001b[39m=\u001b[39m ((config\u001b[39m.\u001b[39mvocab_size \u001b[39m+\u001b[39m \u001b[39m63\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m64\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m    198\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m VocabParallelEmbedding(\n\u001b[1;32m    199\u001b[0m     vocab_size, config\u001b[39m.\u001b[39mhidden_size, perform_initialization\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    200\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([\n\u001b[0;32m--> 201\u001b[0m     LlamaDecoderLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    202\u001b[0m ])\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39m=\u001b[39m RMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/vllm/vllm/model_executor/models/llama.py:146\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mhidden_size\n\u001b[0;32m--> 146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn \u001b[39m=\u001b[39m LlamaAttention(\n\u001b[1;32m    147\u001b[0m     hidden_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_size,\n\u001b[1;32m    148\u001b[0m     num_heads\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnum_attention_heads,\n\u001b[1;32m    149\u001b[0m     num_kv_heads\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnum_key_value_heads,\n\u001b[1;32m    150\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp \u001b[39m=\u001b[39m LlamaMLP(\n\u001b[1;32m    152\u001b[0m     hidden_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size,\n\u001b[1;32m    153\u001b[0m     intermediate_size\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mintermediate_size,\n\u001b[1;32m    154\u001b[0m     hidden_act\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mhidden_act,\n\u001b[1;32m    155\u001b[0m )\n\u001b[1;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm \u001b[39m=\u001b[39m RMSNorm(config\u001b[39m.\u001b[39mhidden_size,\n\u001b[1;32m    157\u001b[0m                                eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/vllm/vllm/model_executor/models/llama.py:118\u001b[0m, in \u001b[0;36mLlamaAttention.__init__\u001b[0;34m(self, hidden_size, num_heads, num_kv_heads)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqkv_proj \u001b[39m=\u001b[39m ColumnParallelLinear(\n\u001b[1;32m    104\u001b[0m     hidden_size,\n\u001b[1;32m    105\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_num_heads \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_num_kv_heads) \u001b[39m*\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m     perform_initialization\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    110\u001b[0m )\n\u001b[1;32m    111\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mo_proj \u001b[39m=\u001b[39m RowParallelLinear(\n\u001b[1;32m    112\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_num_heads \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim,\n\u001b[1;32m    113\u001b[0m     hidden_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     perform_initialization\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    117\u001b[0m )\n\u001b[0;32m--> 118\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn \u001b[39m=\u001b[39m PagedAttentionWithRoPE(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m    119\u001b[0m                                    \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead_dim,\n\u001b[1;32m    120\u001b[0m                                    \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaling,\n\u001b[1;32m    121\u001b[0m                                    rotary_dim\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead_dim,\n\u001b[1;32m    122\u001b[0m                                    num_kv_heads\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_kv_heads)\n",
      "File \u001b[0;32m~/vllm/vllm/model_executor/layers/attention.py:259\u001b[0m, in \u001b[0;36mPagedAttentionWithRoPE.__init__\u001b[0;34m(self, num_heads, head_size, scale, rotary_dim, max_position, base, num_kv_heads)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    250\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    251\u001b[0m     num_heads: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     num_kv_heads: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(num_heads, head_size, scale, num_kv_heads)\n\u001b[1;32m    261\u001b[0m     \u001b[39m# Create the cos and sin cache.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     inv_freq \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m (base\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(torch\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, rotary_dim, \u001b[39m2\u001b[39m) \u001b[39m/\u001b[39m rotary_dim))\n",
      "File \u001b[0;32m~/vllm/vllm/model_executor/layers/attention.py:70\u001b[0m, in \u001b[0;36mPagedAttention.__init__\u001b[0;34m(self, num_heads, head_size, scale, num_kv_heads)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_kv_heads \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_queries_per_kv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_kv_heads\n\u001b[1;32m     69\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_mapping \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrepeat_interleave(\n\u001b[0;32m---> 70\u001b[0m     torch\u001b[39m.\u001b[39;49marange(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_kv_heads, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mint32, device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_queries_per_kv)\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_size \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _SUPPORTED_HEAD_SIZES:\n\u001b[1;32m     74\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhead_size (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_size\u001b[39m}\u001b[39;00m\u001b[39m) is not supported. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSupported head sizes: \u001b[39m\u001b[39m{\u001b[39;00m_SUPPORTED_HEAD_SIZES\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
